Why kafa?
1. Distributed, resilinet architecture, fault tolerent
2. Horizantol scalibility(Scale 100s of broker and milions of message per second)
3. High performance(low letency <10ms)

Fault-tolerance is the process of working of a system in a proper way in spite of the occurrence of the failures in the system.
Resilinet means ability to prevent damage or recover when damage occurs.

Yes, Zookeeper is must by design for Kafka. Because Zookeeper has the responsibility a kind of managing Kafka cluste
Since version 2.8.0, Kafka provides an option to run without ZooKeeper, using a feature called Kafka Raft Metadata mode

Use cases:
----------
Messeging system
Activity tracking,
Gather metrics from diff location
Aplication log gethering
Stream processing
De-couple
Integration with spark
Microservice
-----------------
Kafka components:
=================
Topics, Partitions, offsets

>>Topic: A particular stream of data, like table in database can have many topics and its identified by name.
Its support any message format, sequence of messages is call a data stream.
We can not query data from topic, instead use kafaka producer.

Topic devided into 1-n number partions, message within each partion are ordered have incremented id, called offset.
kafak topics are immutable and data kept inside for limited time (default 1 week)


>>Partions: As the Kafka partitions are containing the data records or messages.
Partiona are made of segments (files) at a time only one segemt is active(for data written)
Data assign randomly to a partions unless key is provideed, menas same keys goes into same partions


Two segments setting:
1. log.segment.size the max size of segemt is 1gb default
2. log.segment.ms the time kafak will wait before commite segment is not full defaul 1 week.

Segment comes with 2 indexs:
1. An offset ot position index: Wher to read message from to find message
2. A Timestamp to offset index: Find message with speciifc timestamp.

>>Offset: The Kafka partitions are containing the data records or messages, each of these messages within a partition needs to be assigned an incremental ID. 
This incremental ID is with respect to the position, the message holds inside the Partition. This specific ID has defined an Offset

Offser are not reusable even previuos message has been deleted, ordered are guranterd within partion not across.


>>Schema Registry:: is a simple concept but it’s really powerful in enforcing data governance within your Kafka architecture. Schemas reside outside of your Kafka cluster,
 only the schema ID resides in your Kafka, hence making schema registry a critical component of your infrastructure. If the schema registry is not available, 
it will break producers and consumers

		    [SchemaRegistry : Apache Avro™ ]

Prodcuer -> shemaid+data [kafak] shemaid+data -> Consumer


With the schema registry in place, the producer, before sending the data to Kafka, talks to the schema registry first and checks if the schema is available. 
If it doesn’t find the schema then it registers and caches it in the schema registry. Once the producer gets the schema, it will serialize the data with the schema 
and send it to Kafka in binary format prepended with a unique schema ID. When the consumer processes this message, it will communicate with the schema registry using 
the schema ID it got from the producer and deserialize it using the same schema. If there is a schema mismatch, the schema registry will throw an error 
letting the producer know that it’s breaking the schema agreement.
 
Kafka Error Handling:
---------------------
Dead letter queue is nothing but creating new failure queue in the name of TOPIC.DLT, If consumer not able to process message even after retry those unprocessed message 
pushed to Dead letter queue.

 Spring boot provides @DltHandler by using this annotation we can able to enable DLT logic for failure message based on Kafka topic.

Data duplicate:
------------------
Duplicate messages can occur in the scenario where:

A Producer attempts to write a message to a topic partition.
The broker does not acknowledge the write due to some transient failure scenario.
The Producer should retry as it does not know whether the write succeeded or not.
If the Producer is not idempotent and the original write did succeed then the message would be duplicated.

Elimination of Message Duplication: The idempotent producer ensures that duplicate messages are prevented from being written to Kafka. 


Ackoledgement stretegy:
----------------------
acks=0 (possible data loss) : Which is the Producer who just sends the data and will not wait for an acknowledgment.
Ex: Metrics collection, Log collection

acks=1 (limited data loss): 

acks=all (no data loss): 
min.insync.replicas=2: That means that at least two brokers that are insync replicas, including the leader, must respond that they have the data.
So, if you want maximum safety, maximum availability, then min.insync.replicas=two, acks=all, and a replication.factor of at least 3.

Avoid duplicate Data:
---------------------
max.inflight.requests = 5
producerProperties.put("enable.idempotence", true)

the acknowledgment never reaches the producer then producer retries again using that request produce ID, the Kafka broker is able to detect that this is a duplicate request. 
And so Kafka is smart and it is not going to commit the same produce request twice, but this time it will send back an acknowledgment saying, “Yes, we got it once already”

Kafaka Producer: Message serialization:
---------------------------------------
Its write data to topics and know which partions to write.
In case of broker failure producer automatically recovred. producer send a message with key, if key==null, data send in roud robin process.
If key!= null, then all message from that key always goes in same partions(by hashing alo murmur2)
So key is only required if we want message orderring for specific field.

Kafak Consumer: Message Deserialization:
----------------------------------------
Read data from topic (pull mechanisam) its choose broker in case broker failure its know how to recover.
Data read in order from low to Heigh offset within each partion.

Kafa Consumer groups:
---------------------
All consumer in application read data as customer group.
Each customer in a group read from exclusive partions(don't allow multiple consumers per partions).
In Kafak multiple customer group in same topic.


Kafa Consumer Offset:
---------------------
Kafaka stores the offset at which a customer group has bean reading.
By default java consumer will automatically commits offset(at least once).
Partionors.class=Roudrobinpartiona.class

Note: Offset like a topic(consumer_offset) that holds info like partion, group, offset(last reading value).
